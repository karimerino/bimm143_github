---
title: "Class 7: Machine Learning 1"
author: "Carolina Merino (PID 14484883)"
format: pdf
editor: visual
---

Today we will explore some fundamental machine learning methods including  clustering and dimensionality reduction.

## K-means clustering

To see how this works let's first make up some data cluster where we know what the answer should be. We can use the `rnorm()` function to help here:

```{r}
hist(rnorm(500, mean = 5))

```

```{r}
x <- c(rnorm(30, mean = -3), rnorm(30, mean = 3))
y <- rev(x)
```

```{r}
x <- c(rnorm(30, mean = -3), rnorm(30, mean = 3))
y <- rev(x)

xy <- cbind(x, y)
plot(x)
```
The function for K-means clustering in "base" R is `kmeans()`

```{r}
k <-kmeans(x, centers= 2)
k
```
To get at the results of the returend list object we can use the dollar `$` syntax

```{r}
k$size
```
>   Q. How many points are in each cluster?

```{r}
k$size

```


>   Q. What 'component' of your result object details
  - cluster assignment/ membership?
  - cluster center?

```{r}
k$cluster
k$centers
```
  
  
  >   Q. Make a clustering results figure of the data colored by cluster membership.

```{r}
plot(xy, col = k$cluster, pch = 16)
points(k$centers, col="blue", pch=16, cex=2)

```

K-means clustering is very popular because it is fast and relatively straightforward: it takes numerical data as input and returns a cluster membership vector, among other results.

The "issue" is we tell `kmeans()` how many clusters we want!

> Q. Run kmeans again and cluster into 4 groups/clusters and plot the results like we did above.


```{r}
k4 <- kmeans(x, centers = 4)
plot(x, col = k4$cluster, pch = 16)
points(k4$centers, pch = 15)

```

Scree Plot


```{r}
# Example data
x <- c(rnorm(30, mean = -3), rnorm(30, mean = 3))
y <- rev(x)
xy <- cbind(x, y)

# Calculate total within-cluster sum of squares for k = 1 to 5
tot_within <- numeric(5)
for (k in 1:5) {
  km <- kmeans(xy, centers = k)
  tot_within[k] <- km$tot.withinss
}


plot(1:5, tot_within, type = "o", 
     pch = 19, col = "blue", lty = 1, lwd = 2,
     xlab = "Number of clusters (k)",
     ylab = "Total within-cluster SS",
     main = "Elbow Plot for k-means Clustering")


```

A scree plot using K-means shows a line of how “tight” the clusters are for different numbers of clusters, helping you pick the best number.


## Hierarchical Clusters

The main "base" R function for Hierarchical Clustering is called `hclust()`. Here we can't just input our data; we first need to calculate the distance matrix (e.g., `dist()`) for our data and use this as input to `hclust()`.


```{r}
d <- dist(x)
hc <- hclust(d)
hc
```
 There is a plot method for hclust results lets try it
 
```{r}
plot(hc)               
abline(h = 8, col = "red")  # at height 8
clusters <- cutree(hc, h = 8)
clusters
```
 
To get our cluster 'membership' vector (i.e., our main clustering result) we can 'cut' the tree at a given height, at a height that yields a given 'k' groups.


```{r}
cutree(hc, h=8)
```

```{r}
grps <- cutree(hc, k=2)
```

> Q. Plot the data with our hclust result coloring

```{r}
plot(x, col=grps)
```


# Principal Component Analysis (PCA)

##PCA of UK food data

Import food data from an online CSV file

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
head(x)
```

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
x
```

```{r}
x <- read.csv(url, row.names = 1)
x
```


Some base figures

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))

```

There is one plot that can be useful for small datasets
```{r}
pairs(x, col=rainbow(10), pch=16)
```

> Main point: It can be difficult to spot major trends and patterns even in relatively small multivariate datasets (here we only have 17 dimensions, typically we have 1000s). So, let's see if PCA can help!

## PCA to the rescue

The main function in "base" R for PCA is called `prcomp()`

I will take the transpose of our data so the "foods" are in the columns:
```{r}
pca <- prcomp( t(x))
summary(pca)
```
>Together PC1 and PC2 captures around ~96% varience of the data, good capture of the data


```{r}
# This is what I want to plot
cols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col = cols, pch = 16,
     xlab = "PC1", ylab = "PC2", main = "PCA Plot")
```



```{r}
library(ggplot2)
```


```{r}
library(ggplot2)

pca_df <- as.data.frame(pca$x)
pca_df$group <- factor(c(1,2,3,4))

ggplot(pca_df, aes(x = PC1, y = PC2, color = group)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("orange", "red", "blue", "darkgreen")) +
  labs(title = "PCA Plot", x = "PC1", y = "PC2")

```


```{r}
library(ggplot2)
food_items <- c("Alcoholic drinks", "Beverages", "Carcase meat", "Cereals",
                "Cheese", "Confectionery", "Fats and oils", "Fish",
                "Fresh fruit", "Fresh potatoes", "Fresh Veg", "Other meat",
                "Other Veg", "Processed potatoes", "Processed Veg", "Soft drinks", "Sugars")

rotation_df <- as.data.frame(pca$rotation)
rotation_df$Food <- food_items

ggplot(rotation_df, aes(x = PC1, y = Food)) +
  geom_col(fill = "steelblue") +
  labs(title = "PC1 Loadings", x = "PC1", y = "Food Items")

```

PCA looks super useful and we will come back to describe this further next day :)